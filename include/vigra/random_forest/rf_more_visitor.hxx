/************************************************************************/
/*                                                                      */
/*        Copyright 2011-2013 by Luca Fiaschi                           */
/*                                                                      */
/*    This file is part of the VIGRA computer vision library.           */
/*    The VIGRA Website is                                              */
/*        http://hci.iwr.uni-heidelberg.de/vigra/                       */
/*    Please direct questions, bug reports, and contributions to        */
/*        ullrich.koethe@iwr.uni-heidelberg.de    or                    */
/*        vigra@informatik.uni-hamburg.de                               */
/*                                                                      */
/*    Permission is hereby granted, free of charge, to any person       */
/*    obtaining a copy of this software and associated documentation    */
/*    files (the "Software"), to deal in the Software without           */
/*    restriction, including without limitation the rights to use,      */
/*    copy, modify, merge, publish, distribute, sublicense, and/or      */
/*    sell copies of the Software, and to permit persons to whom the    */
/*    Software is furnished to do so, subject to the following          */
/*    conditions:                                                       */
/*                                                                      */
/*    The above copyright notice and this permission notice shall be    */
/*    included in all copies or substantial portions of the             */
/*    Software.                                                         */
/*                                                                      */
/*    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND    */
/*    EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES   */
/*    OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND          */
/*    NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT       */
/*    HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,      */
/*    WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING      */
/*    FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR     */
/*    OTHER DEALINGS IN THE SOFTWARE.                                   */
/*                                                                      */
/************************************************************************/

#ifndef RF_MORE_VISITORS_HXX
#define RF_MORE_VISITORS_HXX

#include <map>

#include "rf_visitors.hxx"

namespace vigra
{
namespace rf
{
namespace visitors
{



/** Visitor that calculates different OOB error statistics
 */
class OOB_VariancePerLeaf : public VisitorBase
{
public:
	typedef int tree_index;
	typedef int leaf_index;
	typedef int example_index; // index of the sample in the feature matrix

	typedef std::vector<double> Point;
	typedef std::pair<tree_index, leaf_index> LeafType;
	typedef std::map<LeafType, std::vector<
			example_index> >::iterator KeyIt;
	typedef std::vector<example_index>::iterator ValueIt;

	std::map<LeafType, double> mapping_to_count; // map the tree and the leaf to the average in the leaf
	std::map<LeafType, Point> mapping_to_variance; // map the tree and the leaf to the average in the leaf

	double totalVariance;


	OOB_VariancePerLeaf(): totalVariance(0){};

    template<class RF, class PR, class SM, class ST>
    void visit_after_tree(RF& rf, PR & pr,  SM & sm, ST & st, int index)
    {
    	int nlabels=pr.response().shape(1);
        // go through the samples
        for(int ll = 0; ll < rf.ext_param_.row_count_; ++ll)
        {
            // if the lth sample is oob...
            //if(!sm.is_used()[ll])
            {
                // update number of trees in which current sample is oob
                // get the predicted votes ---> tmp_prob;
            	ArrayVector<double>::const_iterator response;
                int pos =  rf.tree(index).getToLeaf(rowVector(pr.features(),ll));
                response=rf.trees_[index].predict(rowVector(pr.features(),ll));
                LeafType leaf(index,pos);

                if (!mapping_to_variance.count(leaf))
                	{
						Point & sum=mapping_to_variance[leaf];
						sum.resize(rf.ext_param_.class_count_);
						for(int k=0; k<nlabels; ++k)
							sum[k]=0;
						double & nsamples = mapping_to_count[leaf];
						nsamples=0;
                	}

                double & nsamples = mapping_to_count[leaf];
                ++nsamples;

                Point & sum=mapping_to_variance[leaf];
                for(int k=0; k<nlabels; ++k)
                {
//std::cerr << "sample index " << ll << "  prediction " << response[k] << " true " << rowVector(pr.response(),ll)[k] << std::endl;
                	sum[k]+=(response[k]-rowVector(pr.response(),ll)[k])*(response[k]-rowVector(pr.response(),ll)[k]);
                }
			}
        }
    }

    /** Normalise variable importance after the number of trees is known.
     */
    template<class RF, class PR>
    void visit_at_end(RF & rf, PR & pr)
    {
    	//std::cerr << "NORMALIZATIOOOOOONNNN " << std::endl;
		//normalize

    	Point total(rf.ext_param_.class_count_,0);

    	int count=0;
		for (std::map<LeafType, Point >::iterator it=mapping_to_variance.begin();it!=mapping_to_variance.end();it++)
		{	++count;
			Point & sum = mapping_to_variance[(*it).first];
			double & nsamples = mapping_to_count[(*it).first];

			for(int k=0; k<rf.ext_param_.class_count_; ++k)
			{
				sum[k]/=nsamples;
				total[k]+=sum[k];
			}
		}


		for(int k=0; k<rf.ext_param_.class_count_; ++k)
			{
				total[k]/=count;
				this->totalVariance+=total[k];
			}

		std::cerr << "NORMALIZATIOOOOOONNNN " << this->totalVariance << std::endl;
    }
};


/** This visitor is used at prediction time to recall where the current sample finish in the leaf and then cas the votes for the hough forest
 */
class IndexVisitorPrediction: public VisitorBase
{

public:
	typedef int tree_index;
	typedef int leaf_index;
	typedef int example_index; // index of the sample in the feature matrix

	typedef std::map<example_index, std::vector<std::pair<tree_index,
			leaf_index> > >::iterator KeyIt;
	typedef std::vector<std::pair<tree_index, leaf_index> >::iterator ValueIt;

	int current_tree;
	int current_sample;
	std::map<example_index, std::vector<std::pair<tree_index, leaf_index> > >
			mapping; //map tree and leaf with index of the sample

	IndexVisitorPrediction() :
		current_tree(0), current_sample(0)
	{
	}

	/** simply increase the tree count
	 */
	template<class RF, class PR, class SM, class ST>
	void visit_after_tree(RF& rf, PR & pr, SM & sm, ST & st, int index)
	{
		++index;
		current_tree = index;
	}

	template<class RF>
	void visit_before_tree_prediction(RF& rf, int index, int sample_index)
	{
		current_sample = sample_index;
		current_tree = index;
	}

	template<class TR, class IntT, class TopT, class Feat>
	void visit_external_node(TR & tr, IntT index, TopT node_t, Feat & features)
	{
		//std::cout << "current tree " << current_tree << std::endl;
		//std::cout << "leaf address " << index << std::endl;
		//std::cout << "current samples " << current_sample << std::endl;

		//mapping[std::make_pair(current_tree, index)] = std::vector<example_index> () ;
		std::vector<std::pair<tree_index, leaf_index> > & ref =
				mapping[current_sample];

		ref.push_back(std::make_pair(current_tree, index));

		//	++current_sample;
	}

	void save(std::string filen, std::string pathn)
	{
		//FIXME This might be slow;
		ArrayVector<int> temp;

		for (KeyIt it = mapping.begin(); it != mapping.end(); it++)
		{
			int sample = ((*it).first);

			int arr_size = ((*it).second).size();
			//std::cout << "tree " << tree << std::endl;
			temp.push_back(sample);

			temp.push_back(arr_size);
			//std::cout << "leaf " << leaf << std::endl;

			//std::cout << "arr-size " << arr_size << std::endl;

			std::vector<std::pair<tree_index, leaf_index> >& temp_vec =
					(*it).second;

			for (int ii = 0; ii < temp_vec.size(); ii++)
			{
				int tree = temp_vec[ii].first;
				int leaf = temp_vec[ii].second;
				temp.push_back(tree);
				temp.push_back(leaf);
			}
		}

		//std::cout << temp << std::endl;

		//for(int i=0; i<temp.size();i++)
		//	std::cerr<<temp[i]<<",";
		//std::cerr<<std::endl;

		const char* filename = filen.c_str();

		MultiArrayShape<1>::type shp(temp.size());

		//FIXME this actually copy the data
		MultiArray<1u, int> multi(shp);
		multi.reshape(shp);

		for (int i = 0; i < multi.shape(0); i++)
		{
			multi(i) = temp[i];
		}

		writeHDF5(filename, pathn.c_str(), multi);
	}

	void load(std::string filen, std::string pathn)
	{
		HDF5ImportInfo info(filen.c_str(), pathn.c_str());
		vigra_precondition(info.numDimensions() == 1, "Dataset must be 1-dimensional.");

		MultiArrayShape<1>::type shp(info.shape()[0]);
		MultiArray<1, int> array(shp);

		readHDF5(info, array);

		//for (int i=0; i<shp[0];i++)
		//	std::cerr << array(i) << ",";
		//std::cerr<< std::endl;
		//std::cerr<< "array " << array << std::endl;
		mapping.clear();

		for (int it = 0; it < array.shape(0);)
		{
			int current_sample = array(it);
			++it;

			int arr_size = array(it);
			++it;

			std::vector<std::pair<tree_index, leaf_index> > & ref =
					mapping[current_sample];

			//std::cerr << "arr size   " << arr_size << std::endl;

			ref.resize(arr_size);
			int i = 0;
			int current_it = it;
			while (i < arr_size)
			{
				ref[i] = std::make_pair(array(it), array(it + 1));

				++i;
				it = it + 2;
			}
		}
	}

	void deepFout()
	{
		for(KeyIt it=mapping.begin(); it!=mapping.end();++it)
		{
			std::cerr << "sample " << (*it).first << std::endl;

			for (ValueIt ii=((*it).second).begin();ii!=((*it).second).end();++ii)
			{
			 std::cerr << "(  " << (*ii).first << ", " << (*ii).second << ") " ;
			}
			std::cerr << std::endl;
		}
	}

	void clear()
	{
		current_tree = 0;
		current_sample = 0;
		mapping.clear();
	}

	int getNLeafs()
	{
		return mapping.size();
	}
};

class IndexVisitor: public VisitorBase
{

public:
	typedef int tree_index;
	typedef int leaf_index;
	typedef int example_index; // index of the sample in the feature matrix

	typedef std::map<std::pair<tree_index, leaf_index>, std::vector<
			example_index> >::iterator KeyIt;
	typedef std::vector<example_index>::iterator ValueIt;

	int current_tree;
	int current_sample;
	std::map<std::pair<tree_index, leaf_index>, std::vector<example_index> >
			mapping; //map tree and leaf with index of the sample

	IndexVisitor() :
		current_tree(0), current_sample(0)
	{
	}

	/** do something after the the Split has decided how to process the Region
	 * (Stack entry)
	 *
	 * \param tree      reference to the tree that is currently being learned
	 * \param split     reference to the split object
	 * \param parent    current stack entry  which was used to decide the split
	 * \param leftChild left stack entry that will be pushed
	 * \param rightChild
	 *                  right stack entry that will be pushed.
	 * \param features  features matrix
	 * \param labels    label matrix
	 * \sa RF_Traits::StackEntry_t
	 */
	template<class Tree, class Split, class Region, class Feature_t,
			class Label_t>
	void visit_after_split(Tree & tree, Split & split, Region & parent,
			Region & leftChild, Region & rightChild, Feature_t & features,
			Label_t & labels)
	{
		if (tree.isLeafNode(split.createNode().typeID()))
		{

			//the leaf has just been created so the size of the topology gives its address
			leaf_index leaf_addr = tree.topology_.size();

			//std::cout << "current tree " << current_tree << std::endl;
			//std::cout << "node address " << leaf_addr << std::endl;
			//std::cout << "n samples " << parent.end()- parent.begin() << std::endl;

			if (mapping.find(std::make_pair(current_tree, leaf_addr))
					!= mapping.end())
				throw std::runtime_error(
						"Visitor(): Same leaf encountered twice during learning");

#						//mapping[std::make_pair(current_tree, leaf_addr)] = std::vector<example_index> () ;
			std::vector<example_index> & ref = mapping[std::make_pair(
					current_tree, leaf_addr)];
			ref.resize(parent.size());

			typename Region::IndexIterator it = parent.begin();
			int i = 0;
			while (it != parent.end())
			{
				//std::cout << *it << std::endl;
				ref[i] = *it;
				//std::cout << ref[i] << std::endl;
				++it;
				++i;
			}

			//std::copy(parent.begin(), parent.end(), std::back_inserter(ref));
			// std::copy(parent.begin(), parent.end(), ref.begin());
		}
		else
		{
			//std::cout << node_addr << std::endl;
		}
	}

	/** simply increase the tree count
	 */
	template<class RF, class PR, class SM, class ST>
	void visit_after_tree(RF& rf, PR & pr, SM & sm, ST & st, int index)
	{
		++index;
		current_tree = index;
	}

	template<class RF>
	void visit_before_tree_prediction(RF& rf, int index, int sample_index)
	{
		current_sample = sample_index;
		current_tree = index;
	}

	template<class TR, class IntT, class TopT, class Feat>
	void visit_external_node(TR & tr, IntT index, TopT node_t, Feat & features)
	{
		//std::cout << "current tree " << current_tree << std::endl;
		//std::cout << "leaf address " << index << std::endl;
		//std::cout << "current samples " << current_sample << std::endl;

		//mapping[std::make_pair(current_tree, index)] = std::vector<example_index> () ;
		std::vector<example_index> & ref = mapping[std::make_pair(current_tree,
				index)];

		ref.push_back(current_sample);

		++current_sample;
	}

	void save(std::string filen, std::string pathn)
	{
		//FIXME This might be slow;
		ArrayVector<int> temp;

		for (KeyIt it = mapping.begin(); it != mapping.end(); it++)
		{
			int tree = ((*it).first).first;
			int leaf = ((*it).first).second;

			int arr_size = ((*it).second).size();
			//std::cout << "tree " << tree << std::endl;
			temp.push_back(tree);

			temp.push_back(leaf);
			//std::cout << "leaf " << leaf << std::endl;

			temp.push_back(arr_size);

			//std::cout << "arr-size " << arr_size << std::endl;

			std::vector<example_index>& temp_vec = (*it).second;

			temp.insert(temp.end(), temp_vec.begin(), temp_vec.end());

		}

		//std::cout << temp << std::endl;


		const char* filename = filen.c_str();

		MultiArrayShape<1>::type shp(temp.size());

		//FIXME this actually copy the data
		MultiArray<1u, int> multi(shp);
		multi.reshape(shp);

		for (int i = 0; i < multi.shape(0); i++)
		{
			multi(i) = temp[i];
		}

		writeHDF5(filename, pathn.c_str(), multi);

	}

	void load(std::string filen, std::string pathn)
	{
		HDF5ImportInfo info(filen.c_str(), pathn.c_str());
		vigra_precondition(info.numDimensions() == 1, "Dataset must be 1-dimensional.");

		MultiArrayShape<1>::type shp(info.shape()[0]);
		MultiArray<1, int> array(shp);

		readHDF5(info, array);

		mapping.clear();

		for (int it = 0; it < array.shape(0);)
		{

			int current_tree = array(it);
			++it;

			int current_leaf = array(it);
			++it;

			int arr_size = array(it);
			++it;

			std::vector<example_index> & ref = mapping[std::make_pair(
					current_tree, current_leaf)];

			ref.resize(arr_size);
			int i = 0;
			int current_it = it;
			while (it < current_it + arr_size)
			{

				ref[i] = array(it);
				++i;
				++it;
			}
		}
	}

	//DEBUG
	void fout()
	{
		std::map<std::pair<tree_index, leaf_index>, std::vector<example_index> >::iterator
				it = mapping.begin();
		std::cout << std::endl;
		while (it != mapping.end())
		{
			std::cout << "tree number " << (it->first).first << ", "
					<< " leaf index " << (it->first).second << ", "
					<< " nsamples " << (it->second).size() << ", " <<

			std::endl;
			++it;
		}
	}

	void deepFout()
	{
		std::map<std::pair<tree_index, leaf_index>, std::vector<example_index> >::iterator
				it = mapping.begin();

		std::cout << std::endl;
		while (it != mapping.end())
		{
			std::cout << "tree number " << (it->first).first
					<< " leaf address " << (it->first).second << std::endl;

			std::cout << "number samples in the leaf " << (it->second).size()
					<< std::endl;

			std::cout << "the samples indexes are: ";
			for (std::vector<example_index>::iterator i = (it->second).begin(); i
					!= (it->second).end(); i++)
			{
				std::cout << *i << ", ";
			}

			std::cout << std::endl;
			++it;
		}
	}

	void clear()
	{
		current_tree = 0;
		current_sample = 0;
		mapping.clear();
	}

	int getNLeafs()
	{
		return mapping.size();
	}
};


class IndexVisitorTraining: public VisitorBase
{
	/*Visitor Used into the Hough Forest during training time*/

public:
	typedef int tree_index;
	typedef int leaf_index;
	typedef int example_index; // index of the samples in the feature matrix
	typedef std::vector<double> Point; //structure for the point in the matrix includes all the labels with the continuous regression variables

	typedef std::map<std::pair<tree_index, leaf_index>, std::vector<Point> >::iterator
			KeyIt;
	typedef std::vector<Point>::iterator ValueIt;

	int current_tree;
	int current_sample;
	int dim; //This is the size of the space of parameter the usually should be the label matrix -1 column for the categorical variables

	//FIXME: HERE we should use a better method for mapping between leafs and points ... I'm not sure what is the most convenient method
	std::map<std::pair<tree_index, leaf_index>, std::vector<Point> > mapping; //map tree and leaf with index of the sample
	std::map<std::pair<tree_index, leaf_index>, double> mapping_to_prob;

	IndexVisitorTraining(int dim_=2) :
		current_tree(0), current_sample(0), dim(dim_)
	{
	}

	/** do something after the the Split has decided how to process the Region
	 * (Stack entry)
	 *
	 * \param tree      reference to the tree that is currently being learned
	 * \param split     reference to the split object
	 * \param parent    current stack entry  which was used to decide the split
	 * \param leftChild left stack entry that will be pushed
	 * \param rightChild
	 *                  right stack entry that will be pushed.
	 * \param features  features matrix
	 * \param labels    label matrix
	 * \sa RF_Traits::StackEntry_t
	 */
	template<class Tree, class Split, class Region, class Feature_t,
			class Label_t>
	void visit_after_split(Tree & tree, Split & split, Region & parent,
			Region & leftChild, Region & rightChild, Feature_t & features,
			Label_t & labels)
	{
			leaf_index leaf_addr = tree.topology_.size();
			//std::cout << "node address " << leaf_addr << std::endl;
			//std::cout << "current tree " << current_tree << std::endl;
			//std::cout << "n samples " << parent.end()- parent.begin() << std::endl;
		if (tree.isLeafNode(split.createNode().typeID()))
		{
			//the leaf has just been created so the size of the topology gives its address
			if (mapping.find(std::make_pair(current_tree, leaf_addr))
					!= mapping.end())
				throw std::runtime_error(
						"Visitor(): Same leaf encountered twice during learning");

			//mapping[std::make_pair(current_tree, leaf_addr)] = std::vector<Point> () ;

			std::vector<Point> & ref = mapping[std::make_pair(current_tree,
					leaf_addr)];
			double & prob = mapping_to_prob[std::make_pair(current_tree,
					leaf_addr)];

			ref.resize(parent.size());

			typename Region::IndexIterator it = parent.begin();

			double count_pos = 0.0;
			int k = 0;
			while (it != parent.end())
			{

				if (labels(*it, 0) != 0)
				{

					++count_pos;

					//Exclude the first column that was used to define foreground background
					Point p(labels.shape(1) - 1, 0.0);

					//Store the information only for the column in the matrix which have label different from 0
					for (int i = 1; i < labels.shape(1); i++)
					{
						if ((rowVector(labels, *it))[0] != 0)
							p[i - 1] = ((rowVector(labels, *it))[i]);
							if (i==3)
								std::cerr << "The angle for the sample is " << (rowVector(labels, *it))[i] << std::endl;

					}

					ref[k] = p;
					++k;
				}

				++it;

			}
			ref.resize(count_pos);
			prob = count_pos / (double(parent.size())+1e-32);

			//std::copy(parent.begin(), parent.end(), std::back_inserter(ref));
			// std::copy(parent.begin(), parent.end(), ref.begin());
		}
		else
		{
			//std::cout << node_addr << std::endl;
		}
	}

	/** simply increase the tree count
	 */
	template<class RF, class PR, class SM, class ST>
	void visit_after_tree(RF& rf, PR & pr, SM & sm, ST & st, int index)
	{
		current_tree = index + 1;
	}

	template<class RF>
	void visit_before_tree_prediction(RF& rf, int index, int sample_index)
	{
		current_sample = sample_index;
		current_tree = index;
	}

	void save(std::string filen, std::string pathn)
	{
		//FIXME This might be slow save the mapping as a single array
		ArrayVector<double> temp;
		for (KeyIt it = mapping.begin(); it != mapping.end(); it++)
		{
			//we put treee and leaf indeces first
			int current_tree = ((*it).first).first;
			temp.push_back(current_tree);

			int current_leaf = ((*it).first).second;
			temp.push_back(current_leaf);


			// then we push back
			int arr_size = ((*it).second).size();
			temp.push_back(arr_size);

			temp.push_back(dim); //this tells how man parametes were in the three

			double prob = mapping_to_prob[(*it).first];
			temp.push_back(prob);


			//std::cout << "tree " << tree << std::endl;

			//std::cout << "leaf " << leaf << std::endl;


			//std::cout << "arr-size " << arr_size << std::endl;
			const std::vector<Point>& temp_vec=(*it).second;

			for (int i = 0; i < temp_vec.size(); i++)
				temp.insert(temp.end(), temp_vec[i].begin(), temp_vec[i].end());
		}

		//std::cout << temp << std::endl;


		//FIXME: probably in the new version of vigra is possible to avoid this double copy or view the vector as multiarray
		//SAVE to the hdf5
		const char* filename = filen.c_str();
		MultiArrayShape<1>::type shp(temp.size());
		//FIXME this actually copy the data
		MultiArray<1u, double> multi(shp);
		//multi.reshape(shp);

		for (int i = 0; i < multi.shape(0); i++)
		{
			multi(i) = temp[i];
		}

		writeHDF5(filename, pathn.c_str(), multi);
	}

	void load(std::string filen, std::string pathn)
	{
		HDF5ImportInfo info(filen.c_str(), pathn.c_str());
		vigra_precondition(info.numDimensions() == 1, "Dataset must be 1-dimensional.");

		MultiArrayShape<1>::type shp(info.shape()[0]);
		MultiArray<1, double> array(shp);

		readHDF5(info, array);

		mapping.clear();

		for (int it = 0; it < array.shape(0);)
		{
			int current_tree = array(it);
			++it;

			int current_leaf = array(it);
			++it;

			int arr_size = array(it);
			++it;

			int dim = array(it);
			++it;

			double & pr = mapping_to_prob[std::make_pair(current_tree,
					current_leaf)];

			pr = array(it);
			++it;

			std::vector<Point> & ref = mapping[std::make_pair(current_tree,
					current_leaf)];

			ref.resize(arr_size);
			int i = 0;
			int current_it = it;
			while (it < current_it + arr_size * dim)
			{
				for (int k = 0; k < dim; k++)
				{
					ref[i].push_back(array(it));
					++it;
				}

				++i;
			}
		}
	}

	void clear()
	{
		current_tree = 0;
		current_sample = 0;
		mapping.clear();
		mapping_to_prob.clear();
	}

	int getNLeafs()
	{
		return mapping.size();
	}
};

class IndexVisitorTrainingRegressionForest: public VisitorBase
{
	/*Visitor Used into the Hough Forest during training time*/

public:
	typedef int tree_index;
	typedef int leaf_index;
	typedef int example_index; // index of the samples in the feature matrix
	typedef std::vector<double> Point; //structure for the point in the matrix includes all the labels with the continuous regression variables

	typedef std::map<std::pair<tree_index, leaf_index>, std::vector<Point> >::iterator
			KeyIt;
	typedef std::vector<Point>::iterator ValueIt;

	int current_tree;
	int current_sample;
	int dim; //This is the size of the space of parameter the usually should be the label matrix -1 column for the categorical variables
	std::map<std::pair<tree_index, leaf_index>, std::vector<Point> > mapping; //map tree and leaf with index of the sample in the leaf node
	std::map<std::pair<tree_index, leaf_index>, double> mapping_to_prob;

	std::map<std::pair<tree_index, leaf_index>, Point> mapping_to_average; // map the tree and the leaf to the average in the leaf
	std::map<std::pair<tree_index, leaf_index>, Point> mapping_to_variance; // map the tree and the leaf to the variance in the leaf
	std::map<std::pair<tree_index, leaf_index>, double> mapping_to_nsamples; //  map the tree and the leaf to the number of points  in the leaf


	IndexVisitorTrainingRegressionForest(int dim_=2 ) :
		current_tree(0), current_sample(0), dim(dim_)
	{
	}

	/** do something after the the Split has decided how to process the Region
	 * (Stack entry)
	 *
	 * \param tree      reference to the tree that is currently being learned
	 * \param split     reference to the split object
	 * \param parent    current stack entry  which was used to decide the split
	 * \param leftChild left stack entry that will be pushed
	 * \param rightChild
	 *                  right stack entry that will be pushed.
	 * \param features  features matrix
	 * \param labels    label matrix
	 * \sa RF_Traits::StackEntry_t
	 */
	template<class Tree, class Split, class Region, class Feature_t,
			class Label_t>
	void visit_after_split(Tree & tree, Split & split, Region & parent,
			Region & leftChild, Region & rightChild, Feature_t & features,
			Label_t & labels)
	{
			leaf_index leaf_addr = tree.topology_.size();
			//std::cout << "node address " << leaf_addr << std::endl;
			//std::cout << "current tree " << current_tree << std::endl;
			//std::cout << "n samples " << parent.end()- parent.begin() << std::endl;
		if (tree.isLeafNode(split.createNode().typeID()))
		{

			//the leaf has just been created so the size of the topology gives its address
			if (mapping.find(std::make_pair(current_tree, leaf_addr))
					!= mapping.end())
			throw std::runtime_error(
						"Visitor(): Same leaf encountered twice during learning");


			std::vector<Point> & ref = mapping[std::make_pair(current_tree,
					leaf_addr)];

			Point & refAverage  = mapping_to_average[std::make_pair(current_tree,
					leaf_addr)];

			Point & refVariance  = mapping_to_variance[std::make_pair(current_tree,
						leaf_addr)];

			double & nsamples = mapping_to_nsamples[std::make_pair(current_tree,
					leaf_addr)];

			//init the leaf node stored data
			ref.resize(parent.size());
			refAverage.resize(labels.shape(1));
			refVariance.resize(labels.shape(1));

			for (int i = 0; i < labels.shape(1); i++)
			{refAverage[i]=0; refVariance[i]=0;	}



			int k = 0;
			typename Region::IndexIterator it;
			for (it= parent.begin();it != parent.end();it++)
			{
				Point p(labels.shape(1) , 0.0);
					for (int i = 0; i < labels.shape(1); i++)
					{
						p[i] = ((rowVector(labels, *it))[i]);
						refAverage[i]+=p[i];
						refVariance[i]+=p[i]*p[i];
					}
					ref[k] = p;
					++k;
			}

			for (int i = 0; i < labels.shape(1); i++)
			{
				refAverage[i]/=k;
				refVariance[i]=refVariance[i]/k-refAverage[i]*refAverage[i];
			}
			nsamples=k;

		}

	}

	/** simply increase the tree count
	 */
	template<class RF, class PR, class SM, class ST>
	void visit_after_tree(RF& rf, PR & pr, SM & sm, ST & st, int index)
	{
		current_tree = index + 1;
	}

	template<class RF>
	void visit_before_tree_prediction(RF& rf, int index, int sample_index)
	{
		current_sample = sample_index;
		current_tree = index;

	}

	void clear()
	{
		current_tree = 0;
		current_sample = 0;
		mapping.clear();
		mapping_to_prob.clear();
	}

	int getNLeafs()
	{
		return mapping.size();
	}
};

} //namespace visitors
} //namespace rf
} //namespace vigra
#endif
